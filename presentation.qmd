---
title: "Trades With Price Improvement, Always Better Than Market Price?"
author: 
  - Bagas Trilaksonoaji
  - Paul Lajbcygier - Project Supervisor
format: 
  revealjs:
    slide-number: true
    footer: "ETC5543 - Business Analytics Creative Activity"
    theme: default
    transition: fade
    title-slide-attributes: 
      data-background-image: image/background_title.png
      data-background-opacity: "0.6"
editor: visual
width: "100%"
cache: TRUE
---

## Project Summary

<div style="font-size: 60%;">

::: {.fragment}
**Background**: 
    
  - Recent concern from regulator (ASIC) on the violation of `Trade With Price Improvement` rule occurred on the `ASX TRADE` (exchange market for stocks)
  - TWPI rule was introduced in May 2013

:::

::: {.fragment}
**Purpose**: <br>
Understand how effective the rule change was, and understand the pattern of the violation
:::


::: {.fragment}
**Data**: <br>
ASX200 trading data from 2012 - 2015. Source: Securities Industry Research Centre of Asia-Pacific (SIRCA)
:::

::: {.fragment}
**Method**: <br>
Compare TWPI before and after the rule change, then examine the pattern of violation from May 2013 onward 
:::
</div>

<div style="font-size: 90%;">
::: {.fragment}
::: {.callout-tip title="Result"} 
The rule change in 2013 significantly decreased the proportion of `at-the-spread` crossings from 52% to 1.23%. However, the proportion of the violation rose from 1% to 4.5%.
::: 
:::
</div>

## Background - Lit vs Dark Pool

:::: {.columns}

::: {.column width="70%"}
![](image/ASX.PNG){fig-align=center height="100%" width="100%"}
:::

::: {.column width="30%"}
::: {.fragment}
<div style="font-size: 60%;"> 

Lit pool: 

- traders can see the price and volume of all orders, real time
- Ex: ASX trading platform
- establish fair price, transparency

Dark pool:

- details of orders are not disclosed, until transactions are completed
- Ex: brokers internal crossing system (reported to ASX)
- cause less fluctuation to the market

</div> 
:::

:::
::::

## Background - Trade With Price Improvement

Rule: Equity market products (stocks) must be traded on **lit market**, like ASX trading platform. Except ...

:::: {.columns}

::: {.column width="55%"}


::: {.fragment}

<div style="font-size: 26px;">
1. block trade: large volume
2. portfolio trade: a number of different stocks, on a single agreement
3. out-of-hours trade: outside open-session 
4. **Trade with price improvement**:

    - crossings (trades within a broker) at a better price than what is available on the lit-market 
    - benefit the traders
    - reported, but not executed on ASX. Hence, can be violated, e.g. reported as TWPI but actually traded at the spread 
</div>

:::

:::

::: {.column width="45%"}

::: {.fragment}
<img data-src="image/orderbook2.jpg" height="400" width="700" />
:::

:::

::::


## Research Problem

:::: {.columns}

::: {.column width="65%"}

::: {style="font-size:30px;"}

1. Before TWPI, `Crossings at or within the spread` rule was introduced in the late 2011. 
2.  Amendment in May 2013 $\rightarrow$ `Trade with price improvement`
    - the only difference: exclude `at-the-spread` crossings
    - more trading in the lit market, ensuring better price and liquidity - [ASIC Report 394](https://asic.gov.au/about-asic/news-centre/find-a-media-release/2014-releases/14-105mr-asic-reports-on-dark-liquidity-rules/)
    - Other POV: ASX as a for-profit organisation seeks profit $\rightarrow$ Fees from trading on their platform 
3.  **Recent finding by ASIC: violation of TWPI rule**

:::

:::

::: {.column width="35%"}

<br>

::: {.fragment}
<img data-src="image/wilson.png" height="350" width=700" />
:::

:::

::::

## Key Questions

1. Is there any significant decrease in the number of `crossings at the spread` after the rule amendment on May 2013?
2. How many `TWPI` violations occurred after the rule change?
3. Who did the violations?
4. What conditions contributed to the occurrence of the violations? Develop a model ...

## Data

- ASX200 data from 2012 to 2015
- Source: Securities Industry Research Centre of Asia-Pacific (SIRCA)
- Extracted with Nectar Cloud Computing
- 10Million+ of `TWPI` observations.

Nectar Cloud Computing

::: {style="font-size:30px;"}
-   Developed and maintained by Australian Research Data Commons (ARDC)
-   Provide large scale computing infrastructure
:::

## Data

```{r}
library(tidyverse)
library(hms)
library(knitr)
library(kableExtra)
library(tidymodels)
library(discrim)
library(rpart)
library(rpart.plot)
library(randomForest)
library(themis)
```

```{r}
all_1pg <- read.csv("data/1pg-all-dates.csv", colClasses = c(BidID = "character", AskID = "character"))
l1 <- read.csv("data/1pg-L1-data.csv")

all_1pg_session <- all_1pg |>
  mutate(RecordDate = lubridate::ymd(RecordDate),
         hms = hms::as_hms(HourMinuteSecond)) |>
  filter(hms > as_hms("10:00:00"),
         hms < as_hms("16:00:00"))

l1_lag <- l1 |>
  mutate(lagged_bid = lag(L1BidPrice),
         lagged_ask = lag(L1AskPrice),
         RecordDate = lubridate::ymd(RecordDate)) |>
  select(RecordDate, HourMinuteSecond, MilliSecond, lagged_bid, lagged_ask)


all_1pg_best <- all_1pg_session |>
  left_join(l1_lag, by = c("RecordDate","HourMinuteSecond","MilliSecond")) |>
  fill(lagged_bid, .direction = "up") |>
  fill(lagged_ask, .direction = "up") |>
  rename(bid_before = lagged_bid,
         ask_before = lagged_ask)
```

::: columns
::: column
[Trading Data]{style="font-size: 20px; color: red;"}

```{r}

head(all_1pg_session) |> 
  select(-Instrument, -TransID, -(15:19), -(22:23)) |>
  kbl() |>
  kable_styling(font_size = 14, full_width = F, position = "left") 


```
:::

::: column
[Best Bid/Ask Data]{style="font-size: 20px; color: red;"}

```{r}
head(l1) |>
  select(-Instrument) |>
  kbl() |>
  kable_styling(font_size = 14, full_width = F, position = "left") 
```
:::
:::

[Combined Data]{style="font-size: 20px; color: red;"}

```{r}
head(all_1pg_best) |> 
  select(-Instrument, -TransID, -(15:19), -(22:24)) |>
  kbl() |>
  kable_styling(font_size = 14, full_width = F, position = "left")
```

## Result and Discussion

```{r}
# load("data/twpi_2012-2015")

```

::: columns
::: {.column width="33%"}
```{r}
# twpi <- twpi_df |> select(-Count, -(6:16), -(19:21))

# twpi2 <- twpi |>
#   mutate(RecordDate = lubridate::ymd(RecordDate),
#          Hour = substr(HourMinuteSecond, 1, 2),
#          Year = lubridate::year(RecordDate),
#          Month = lubridate::month(RecordDate),
#          Date = lubridate::day(RecordDate),
#          Dayofweek = lubridate::wday(RecordDate)) |>
#   select(1:3, 5:9, 11:15, Marks) |>
#   mutate(BuyerBrokerID = as.factor(BuyerBrokerID),
#          across(9:11, as.factor),
#          across(13:14, as.factor)) |>
#   group_by(Instrument) |>
#   mutate_if(is.numeric, function(x) (x-mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE))
# 
# twpi3 <- twpi2 |> # remove duplicates
#   group_by(Mykey) |>
#   filter(n() == 1) |>
#   ungroup()
# 
# save(twpi3, file = "data/twpi3")
```

```{r}
load("data/twpi3")
```

```{r, eval = FALSE}
# # find the greatest NX XT
twpi3$Instrument |> table() |> sort(decreasing = TRUE)
```


```{r}
rio <- twpi3 ## |> filter(Instrument == "RIO")

rio_bf <- rio |>
  filter(RecordDate < lubridate::ymd("20130526"))

rio_af <- rio |>
  filter(RecordDate >= lubridate::ymd("20130526"))

rio_bf |>
  count(Marks) |>
  mutate(Count = n,
         Percentage = round(Count / sum(Count) * 100, 2)) |>
  select(Marks, Count, Percentage) |>
  left_join(rio_af |>
    count(Marks) |>
    mutate(Count = n,
           Percentage = round(Count / sum(Count) * 100, 2)) |>
    select(Marks, Count, Percentage), by = "Marks") |>
  select(Marks, Percentage.x, Percentage.y) |>
  rename(Before = Percentage.x,
         After = Percentage.y) |>
  mutate(Marks = c("Within", "At Spread", "Outside")) |>
  kbl() |>
  kable_styling(font_size = 20, full_width = FALSE, position = "left") |>
  add_header_above(c(" " = 1, "PERCENTAGE" = 2))
```

```{r}
broker <- read.csv("data/BrokerClassifications.csv")
class <- read.csv("data/Classifications.csv")
broker2 <- broker |> left_join(class, by = "Classification") |>
  mutate(No = as.factor(No))
```

```{r}
broker_table <- rio_af |>
  left_join(broker2, by = c("BuyerBrokerID" = "No")) |>
  group_by(Description) |>
  count(Description) |>
  ungroup() |>
  left_join(
    (
    rio_af |>
    left_join(broker2, by = c("BuyerBrokerID" = "No")) |>
    filter(Marks != 1)  |>
    group_by(Description) |>
    count(Description) |>
    ungroup()
    ), 
    by = "Description") |>
  mutate(Percent = round((n.y/n.x) * 100, 2)) |>
  rename(TWPI = n.x,
         Violation = n.y,
         `Broker Type` = Description) |>
  # arrange(desc(TWPI)) |>
  mutate_if(is.numeric, ~ replace_na(., 0)) |>
  mutate_if(is.character, ~ replace_na(., "Unclassified"))

broker_table |>
  mutate_if(is.numeric, comma) |>
  kbl(align = c("l", "r", "r", "c")) |>
  kable_styling(font_size = 20, full_width = F, position = "left")


```
:::

::: {.column width="67%"}

::: {.fragment}
::: {style="font-size: 25px;"}
- Significant decrease in the number of `at-the-spread` crossings after the rule change, dropping from 52% to 1.23% $\rightarrow$ more trades on ASX platform, more fees
- Violation of the rule increased from 1.05% to approximately 4.46% after the rule change $\rightarrow$ still marginal
- HFT Brokers have done the most TWPI, as well as the most violation. Look closer at individual brokers (Top 10):
:::
:::

::: {.fragment}
```{r}
#| fig-height: 4

broker_10 <- rio_af |> 
  count(BuyerBrokerID) |>
  arrange(desc(n)) |>
  head(10) |>
  pull(BuyerBrokerID)

rio_af |> 
  filter(BuyerBrokerID %in% broker_10)|>
  filter(Marks != 1) |>
  mutate(Marks = ifelse(Marks == 2, "At Spread", "Outside")) |>
  group_by(Marks) |>
  count(BuyerBrokerID) |>
  arrange(desc(n)) |>
  ggplot(aes(x = reorder(BuyerBrokerID, -n), y = n, fill = Marks)) +
  geom_col(position = "dodge") +
  theme_bw() +
  labs(title = "Crossing Trades Reported as TWPI",
       x = "Broker ID",
       y = NULL) +
  theme(legend.position = "right")
```
:::

:::
:::

## Modelling

```{r, include=FALSE}
# prepare and cleaning the data 

# data <- all_1pg_best |> 
#   left_join(twpi[, c("mykey", "code")], by = "mykey") |>
#   left_join(broker2[, c("No", "Description")], by = c("BuyerBrokerID" = "No")) |>
#   filter(RecordType %in% c("TRADE", "OFFTR")) |>
#   mutate(nxxt = ifelse(is.na(code), 0, 1),
#          violation = ifelse(code %in% c("at best", "outside"), 1, 0),
#          year = lubridate::year(RecordDate),
#          month = lubridate::month(RecordDate),
#          date = lubridate::day(RecordDate),
#          dayofweek = lubridate::wday(RecordDate), 
#          hour = lubridate::hour(hms)) |>
#   select(7:8, 10, 20, 27:35)
# 
# data_twpi <- data |>
#   filter(nxxt == 1,
#          !(month %in% c(1,2,3))) |> # month 1 to 3 only have 1 observation
#   mutate(across(6:10, as.factor),
#          across(12:13, as.factor)) |>
#   mutate_if(is.numeric, function(x) (x-mean(x))/sd(x)) |>
#   select(Price, Volume, DollarValue, Description, month, date, dayofweek, hour, violation) |>
#   na.omit()

data <- rio_af |> 
  filter(BuyerBrokerID %in% c(210,361,203,150,231),
         Hour != 16) |>
  # left_join(broker2[, c("No", "Description")], by = c("BuyerBrokerID" = "No")) |>
  mutate(Violation = as.factor(ifelse(Marks == 1, 0, 1))) |>
  select(2, 4, 6:13, 15) |>
  na.omit()

set.seed(1010)
df_split <- initial_split(data, 2/3, strata = Violation)
df_train <- training(df_split)
df_test <- testing(df_split)
```

```{r, eval=FALSE}
#logistic regression
log_mod <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification") |>
  translate()
log_fit <- log_mod |>
  fit(Violation ~ .,
      data = df_train[-1])

# saveRDS(log_fit, file = "data/model/log_fit.rds")


tidy(log_fit) 
glance(log_fit)
```

```{r}
log_fit <- readRDS("data/model/log_fit.rds")
```

```{r,include=FALSE}
# measure accuracy logistic regression
treshold <- 0.05

df_tr_pred <- log_fit |> 
  augment(new_data = df_train) |>
  mutate(pred_viol = as.factor(ifelse(.pred_1 > treshold, 1, 0)))
df_ts_pred <- log_fit |> 
  augment(new_data = df_test) |>
  mutate(pred_viol = as.factor(ifelse(.pred_1 > treshold, 1, 0)))

matrix_tr <- df_tr_pred |>
  count(Violation, pred_viol) |>
  group_by(Violation) |>
  mutate(cl_acc = n[pred_viol == Violation]/sum(n)) |>
  pivot_wider(names_from = pred_viol, 
              values_from = n, values_fill=0)

matrix_ts <- df_ts_pred |>
  count(Violation, pred_viol) |>
  group_by(Violation) |>
  mutate(cl_acc = n[pred_viol == Violation]/sum(n)) |>
  pivot_wider(names_from = pred_viol, 
              values_from = n, values_fill=0)

accuracy(df_tr_pred, Violation, pred_viol)$.estimate
bal_accuracy(df_tr_pred, Violation, pred_viol)$.estimate

accuracy(df_ts_pred, Violation, pred_viol)$.estimate
bal_accuracy_logistic <- bal_accuracy(df_ts_pred, Violation, pred_viol)$.estimate
```

```{r, include = FALSE}
# # LDA
# lda_mod <- discrim_linear() |>
#   set_mode("classification") |>
#   set_engine("MASS", prior = c(0.5, 0.5))
# lda_fit <- lda_mod |> 
#   fit(Violation ~ .,
#       data = df_train[-1])
# 
# lda_fit$fit$scaling
```

```{r, include = FALSE}
# # measure accuracy LDA
# 
# df_tr_pred_lda <- df_train |>
#   mutate(pred_viol = predict(lda_fit$fit, df_train)$class)
# df_ts_pred_lda <- df_test |>
#   mutate(pred_viol = predict(lda_fit$fit, df_test)$class)
# 
# matrix_tr_lda <- df_tr_pred_lda |>
#   count(violation, pred_viol) |>
#   group_by(violation) |>
#   mutate(cl_acc = n[pred_viol == violation]/sum(n)) |>
#   pivot_wider(names_from = pred_viol, 
#               values_from = n, values_fill=0)
# 
# matrix_ts_lda <- df_ts_pred_lda |>
#   count(violation, pred_viol) |>
#   group_by(violation) |>
#   mutate(cl_acc = n[pred_viol == violation]/sum(n)) |>
#   pivot_wider(names_from = pred_viol, 
#               values_from = n, values_fill=0)
# 
# accuracy(df_tr_pred_lda, violation, pred_viol)$.estimate
# bal_accuracy(df_tr_pred_lda, violation, pred_viol)$.estimate
# 
# accuracy(df_ts_pred_lda, violation, pred_viol)$.estimate
# bal_accuracy_lda <- bal_accuracy(df_ts_pred_lda, violation, pred_viol)$.estimate
```

```{r, eval = FALSE}
# decision tree
tree <- decision_tree(
  cost_complexity = 1e-10,
  tree_depth = 10,
  min_n = 4) |>
  set_mode("classification") |>
  set_engine("rpart")

fit_tree <- tree |>
  fit(Violation ~ .,
      data = df_train[-1])

# saveRDS(fit_tree, file = "data/model/fit_tree.rds")

# fit_tree |>
#   extract_fit_engine() |>
#   rpart.plot(type=3, extra=1)
```

```{r}
fit_tree <- readRDS("data/model/fit_tree.rds")
```

```{r, eval=FALSE}
tree_tune <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) |>
  set_mode("classification") |>
  set_engine("rpart")

# create all combination with 5 samples each variable 
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(),
                          levels = 5)

# create cross-validation folds from training set
d_folds <- vfold_cv(df_train)

# tuning the model with all the combination
tree_wf <- workflow() %>%
  add_model(tree_tune) %>%
  add_formula(Violation ~ . -Instrument)

tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = d_folds,
    grid = tree_grid
    )

# select best combination based on accuracy
best_tree <- tree_res |> select_best(metric = "roc_auc")
```

```{r, include=FALSE}
# measure accuracy decision tree
df_tr_pred_tree <- df_train |>
  mutate(pviol = predict(fit_tree$fit, 
                            df_train, 
                            type="class"))

df_ts_pred_tree <- df_test |>
  mutate(pviol = predict(fit_tree$fit, 
                            df_test, 
                            type="class"))

cf_tr <- df_tr_pred_tree |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n)


cf_ts <- df_ts_pred_tree |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n)

accuracy(df_tr_pred_tree, Violation, pviol)
bal_accuracy(df_tr_pred_tree, Violation, pviol)

accuracy(df_ts_pred_tree, Violation, pviol)
bal_accuracy(df_ts_pred_tree, Violation, pviol)
```

```{r, eval=FALSE}
# random forest
set.seed(1010)
rf <- rand_forest(mtry=2, trees=1000) |>
  set_mode("classification") |>
  set_engine("randomForest", importance = TRUE)

rf_workflow <- workflow() |>
  add_model(rf) |>
  add_formula(Violation ~ . -Instrument)

fit_rf <- rf_workflow |> 
  fit(data = df_train)

# fit_rf <- rf |>
#   fit(Violation ~ . -Price,
#       data = df_train)

df_ts_pred_rf <- df_test |>
  mutate(pviol = predict(fit_rf,
                         df_test)$.pred_class)

cf_rf_ts <- df_ts_pred_rf |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n, values_fill = 0)

accuracy(df_ts_pred_rf, Violation, pviol)
bal_accuracy_rf <- bal_accuracy(df_ts_pred_rf, Violation, pviol)$.estimate


rf_model <- pull_workflow_fit(fit_rf)$fit
importance(rf_model)
```

```{r, eval = FALSE}
data_rec <- recipe(Violation ~ ., data = df_train) |>
  step_downsample(Violation) |>
  step_rm(Instrument)

data_prep <- prep(data_rec)
data_juice <- juice(data_prep)

# saveRDS(data_prep, file = "data/model/data_prep.rds")

```

```{r}
data_prep <- readRDS("data/model/data_prep.rds")
data_juice <- juice(data_prep)
```

```{r, eval = FALSE}
# tuning hyperparameters
rf_tune <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()) |>
  set_mode("classification") |>
  set_engine("randomForest")

tune_wf <- workflow() |>
  add_recipe(data_rec) |>
  add_model(rf_tune)
  
doParallel::registerDoParallel() # for paralle processing

set.seed(3344)
w_folds <- vfold_cv(data)

rf_grid <- grid_regular(mtry(range = c(2, 8)),
                        min_n(range = c(1, 4)),
                        levels = 3)

tune_res <- 
  tune_grid(tune_wf,
            resamples = w_folds,
            grid = rf_grid
            )

# select best combination based on accuracy
best_hp <- tune_res |> select_best(metric = "accuracy")
```

```{r, eval = FALSE}
# apply the model
set.seed(3344)
rf <- rand_forest(mtry = 5, 
                     min_n = 4) |>
  set_mode("classification") |>
  set_engine("randomForest", importance = TRUE)

rf_workflow2 <- workflow() |>
  add_model(rf) |>
  add_formula(Violation ~ .)

fit_rf2 <- rf_workflow2 |> 
  fit(data = data_juice)

# saveRDS(fit_rf2, file = "data/model/fit_rf2.rds")
```

```{r}
fit_rf2 <- readRDS("data/model/fit_rf2.rds")
```

```{r, eval = FALSE}
df_ts_pred_rf2 <- bake(data_prep, df_test) |>
  mutate(pviol = predict(fit_rf2,
                         bake(data_prep, df_test))$.pred_class)

# save(df_ts_pred_rf2, file = "data/model/df_ts_pred_rf2.csv")
```

```{r}
load("data/model/df_ts_pred_rf2.csv")

cf_rf_ts2 <- df_ts_pred_rf2 |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n, values_fill = 0)
```

```{r, include = FALSE}
accuracy(df_ts_pred_rf2, Violation, pviol)
bal_accuracy_rf2 <- bal_accuracy(df_ts_pred_rf2, Violation, pviol)$.estimate

rf_model2 <- pull_workflow_fit(fit_rf2)$fit
importance(rf_model2)
```

```{r, eval=FALSE}
# boosted tree
bt <- boost_tree() |>
  set_mode("classification") |>
  set_engine("xgboost")
fit_bt <- bt |>
  fit(Violation ~ .,
      data = df_train[-1])

df_ts_pred_rf <- df_test |>
  mutate(pviol = predict(fit_bt, 
                            df_test)$.pred_class)

cf_bt_ts <- df_ts_pred_rf |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n, values_fill = 0)

bt_acc_ts <- accuracy(df_ts_pred_rf, Violation, pviol)
bt_bacc_ts <- bal_accuracy(df_ts_pred_rf, Violation, pviol)
rbind(bt_acc_ts, bt_bacc_ts)
```

- Only TWPI data from the top 5 brokers is used to model the **violation**, due to their dominant proportion.
- Preprocessing and cleaning the data, include data scaling
- Selecting relevant variables
- The independent variable is binary, violation or non-violation
- Dependent variables:

```{r}
data |> select(-Instrument, -Violation) |> names()
```

## Modelling 

We try four machine learning modelling to understand violation pattern:

-   Logistic Regression
-   Decision Tree
-   Random Forest
-   Boosted Tree (xgboost)

**Random Forest gives the best result for this classification problem: highest balance accuracy score on the test set.**

## Modelling

:::: columns
::: {.column width="35%"}

```{r}
matrix_ts |>
  kbl() |>
  kable_styling(font_size = 20, full_width = F, position = "left") |>
  add_header_above(c("Logistic Regression" = ncol(matrix_ts)))
```

::: {style="font-size: 1em;"}
```{r, echo = TRUE}
bal_accuracy_logistic
```
:::

<br> 

```{r}
cf_rf_ts2 |>
  kbl() |>
  kable_styling(font_size = 20, full_width = F, position = "left") |>
  add_header_above(c("Random Forest" = ncol(matrix_ts)))
```

::: {style="font-size: 1em;"}
```{r, echo = TRUE}
bal_accuracy_rf2
```
:::
:::

::: {.column width="65%" .fragment}
```{r}
# tidy(log_fit)[,1:2] |>
#   left_join(data.frame(lda_fit$fit$scaling) |> rownames_to_column(),
#             by = c("term" = "rowname")) |>
#   rename(logreg_est = estimate,
#          lda_coef = LD1) |>
#   kbl() |>
#   kable_styling(font_size = 15, full_width = F, position = "left")
```
<div style="font-size: 60%;">
Variables importance from the Random Forest Model:
</div>

```{r}
importance(rf_model2) |>
  kbl() |>
  kable_styling(font_size = 18, full_width = T, position = "left") 
```

<div style="font-size: 60%;">
- `Ask/BidVol_before` refers to the volume of the best ask/bid price in the lit market. They have the highest Gini coefficient, indicating their importance in distinguishing between violation and non-violation.
- `DollarValue`, `Volume`, `Date` are also important factors in differentiating result.
- Meanwhile, `Year` and `Month` play a role in enhancing the accuracy of the model.
</div>

:::
::::

## Conclusion

- The purpose of the rule amendment in May 2013 has been met, as the number of `at-the-spread` crossings has drastically reduced. This type of trades moves to ASX platform. 
- Violation of `TWPI` can be associated with the depth of the liquidity  available just before the trades. Date, Volume, and the transaction values also play important roles in the `TWPI` violation pattern.
- There are no measures from ASX to reject `TWPI` reports if the conditions are not met $\rightarrow$ ASX need to address this issue

## Future Work

- Analyse violation pattern with other variables, such as stock industry sector, market cap, lagged price values, bid-ask spread, etc.
- Try different methods to analyse the pattern, such as PCA and factor modelling.
- More research questions:
    
    - How much profit did ASX make due to the rule amendment in May 2013?
    - Why do only a few brokers dominate the TWPI?

# Thank You .. {background-image="image/background_title.png"}


