---
title: "Trades With Price Improvement"
format: 
  revealjs:
    slide-number: true
editor: visual
width: "100%"
cache: TRUE
---

## Project Summary

<div style="font-size: 70%;">
**Background**: 
    
  - Recent concern from regulator (ASIC) on the violation of `Trade With Price Improvement` rule occurred on the `ASX TRADE` (exchange market for stocks)
  - TWPI rule amended in May 2013

**Purpose**: <br>
Understand how effective is the rule change, and understand the pattern of the violation

**Data**: <br>
ASX200 trading data from 2012 - 2015, source: Securities Industry Research Centre of Asia-Pacific (SIRCA)

**Method**: <br>
Compare TWPI before and after the rule change, then finding the pattern of violation on May 2013 onward 

**Result**: <br>
TWPI amendment in 2013 did significantly decrease the proportion of `at-market-price` TWPI from 52% to 1.23%. However, the proportion of the violation rose from 1% to 4.5%.
</div>

## What is TWPI? Background ..

Rule: Equity market products (stocks) must be traded on an **open market**, like ASX trading platform.

<span style="font-size: 80%;"> Open market / lit market: </span> 
    
    -> traders can see the price and volume of all orders on a trading platform, real time 
    -> ASX trading platform
    -> establish fair price, transparency

<span style="font-size: 80%;"> Dark market: </span>

    -> details of orders are not disclosed to the public, until a transaction is completed
    -> brokers internal system (reported to ASX)
    -> less fluctuation to the market

<div style="height: 20px;"></div>

<span style="color: red;">**Both "markets" are just trading venues, they are still in a same market: <br>
ASX TRADE**</span>

## What is TWPI? Definition ..

:::: {.columns}

When the stocks can be traded in a non-open market (dark market)?

::: {.column width="60%"}

<div style="font-size: 30px;">
1. block trade: large volume
2. portfolio trade: a number of different stocks, on a single agreement
3. out-of-hours trade: outside open-session 
4. **Trade with price improvement**:
    
    - Better price than the price on the open-market 
    - benefit traders

</div>

:::

::: {.column width="40%"}

<img data-src="image/orderbook.jpg" height="400" width="400" />

:::

::::


## Research Problem

::: {style="font-size:30px;"}

Background:

1. `Crossings at or within the spread` rule was introduced in the late 2011
2.  Amendment in May 2013 -> `Trade with price improvement`
    -   more trading in the lit market, ensuring better price and liquidity - [ASIC Report 394](https://asic.gov.au/about-asic/news-centre/find-a-media-release/2014-releases/14-105mr-asic-reports-on-dark-liquidity-rules/)
    -   Other POV: ASX as a public owned and for-profit organisation seeks for profit -> Fees from trading on their platform 
    
    
3.  ASIC review of TWPI in 2022:
    -   TWPI reported not exactly occurred at better prices.

:::

## Key Questions

- Decline in the `crossings at best price` after 26 May 2013?
- How many violations on TWPI rule after the rule change?
- Who did the violation?
- What conditions that make the violation occurred? Develop a model ...



## Data

- ASX200 data from 2012 to 2015
- Source: SIRCA
- Extracted with Nectar Cloud Computing

Nectar Cloud Computing

::: {style="font-size:30px;"}
-   aaaaa
-   bbbbb
:::

## Data

```{r}
library(tidyverse)
library(hms)
library(knitr)
library(kableExtra)
library(tidymodels)
library(discrim)
library(rpart)
library(rpart.plot)
library(randomForest)
library(themis)
```

```{r}
all_1pg <- read.csv("data/1pg-all-dates.csv", colClasses = c(BidID = "character", AskID = "character"))
l1 <- read.csv("data/1pg-L1-data.csv")

all_1pg_session <- all_1pg |>
  mutate(RecordDate = lubridate::ymd(RecordDate),
         hms = hms::as_hms(HourMinuteSecond)) |>
  filter(hms > as_hms("10:00:00"),
         hms < as_hms("16:00:00"))

l1_lag <- l1 |>
  mutate(lagged_bid = lag(L1BidPrice),
         lagged_ask = lag(L1AskPrice),
         RecordDate = lubridate::ymd(RecordDate)) |>
  select(RecordDate, HourMinuteSecond, MilliSecond, lagged_bid, lagged_ask)


all_1pg_best <- all_1pg_session |>
  left_join(l1_lag, by = c("RecordDate","HourMinuteSecond","MilliSecond")) |>
  fill(lagged_bid, .direction = "up") |>
  fill(lagged_ask, .direction = "up") |>
  rename(bid_before = lagged_bid,
         ask_before = lagged_ask)
```

::: columns
::: column
[Trading Data]{style="font-size: 20px; color: red;"}

```{r}

head(all_1pg_session) |> 
  select(-Instrument, -TransID, -(15:19), -(22:23)) |>
  kbl() |>
  kable_styling(font_size = 14, full_width = F, position = "left") 


```
:::

::: column
[L1 Data]{style="font-size: 20px; color: red;"}

```{r}
head(l1) |>
  select(-Instrument) |>
  kbl() |>
  kable_styling(font_size = 14, full_width = F, position = "left") 
```
:::
:::

[Combined Data]{style="font-size: 20px; color: red;"}

```{r}
head(all_1pg_best) |> 
  select(-Instrument, -TransID, -(15:19), -(22:24)) |>
  kbl() |>
  kable_styling(font_size = 14, full_width = F, position = "left")
```

## Challenges

-   Latest open data available only until 2016
-   Handling large amount of data need huge processing power
-   Nectar can handle it, but ...
    -   still need days
    -   new issues every weeks

## Result and Discussion

```{r}
# load("data/twpi_2012-2015")

```

::: columns
::: {.column width="32%"}
```{r}
# twpi <- twpi_df |> select(-Count, -(6:16), -(19:21))

# twpi2 <- twpi |>
#   mutate(RecordDate = lubridate::ymd(RecordDate),
#          Hour = substr(HourMinuteSecond, 1, 2),
#          Year = lubridate::year(RecordDate),
#          Month = lubridate::month(RecordDate),
#          Date = lubridate::day(RecordDate),
#          Dayofweek = lubridate::wday(RecordDate)) |>
#   select(1:3, 5:9, 11:15, Marks) |>
#   mutate(BuyerBrokerID = as.factor(BuyerBrokerID),
#          across(9:11, as.factor),
#          across(13:14, as.factor)) |>
#   group_by(Instrument) |>
#   mutate_if(is.numeric, function(x) (x-mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE))
# 
# twpi3 <- twpi2 |> # remove duplicates
#   group_by(Mykey) |>
#   filter(n() == 1) |>
#   ungroup()
# 
# save(twpi3, file = "data/twpi3")
```

```{r}
load("data/twpi3")
```

```{r, eval = FALSE}
# # find the greatest NX XT
twpi3$Instrument |> table() |> sort(decreasing = TRUE)
```


```{r}
rio <- twpi3 ## |> filter(Instrument == "RIO")

rio_bf <- rio |>
  filter(RecordDate < lubridate::ymd("20130526"))

rio_af <- rio |>
  filter(RecordDate >= lubridate::ymd("20130526"))

rio_bf |>
  count(Marks) |>
  mutate(Count = n,
         Percentage = round(Count / sum(Count) * 100, 2)) |>
  select(Marks, Count, Percentage) |>
  left_join(rio_af |>
    count(Marks) |>
    mutate(Count = n,
           Percentage = round(Count / sum(Count) * 100, 2)) |>
    select(Marks, Count, Percentage), by = "Marks") |>
  select(Marks, Percentage.x, Percentage.y) |>
  rename(Before = Percentage.x,
         After = Percentage.y) |>
  kbl() |>
  kable_styling(font_size = 20, full_width = F, position = "left")
```

```{r}
broker <- read.csv("data/BrokerClassifications.csv")
class <- read.csv("data/Classifications.csv")
broker2 <- broker |> left_join(class, by = "Classification") |>
  mutate(No = as.factor(No))
```

```{r}
broker_table <- rio_af |>
  left_join(broker2, by = c("BuyerBrokerID" = "No")) |>
  group_by(Description) |>
  count(Description) |>
  ungroup() |>
  left_join(
    (
    rio_af |>
    left_join(broker2, by = c("BuyerBrokerID" = "No")) |>
    filter(Marks != 1)  |>
    group_by(Description) |>
    count(Description) |>
    ungroup()
    ), 
    by = "Description") |>
  mutate(Percentage = round((n.y/n.x) * 100, 2)) |>
  rename(TWPI = n.x,
         Violation = n.y,
         `Broker Type` = Description) |>
  # arrange(desc(TWPI)) |>
  mutate_if(is.numeric, ~ replace_na(., 0)) |>
  mutate_if(is.character, ~ replace_na(., "Unclassified"))

broker_table |>
  kbl() |>
  kable_styling(font_size = 20, full_width = F, position = "left")


```
:::

::: {.column width="68%"}
::: {style="font-size: 30px;"}
-   13% of reported TWPIs violated the rule
-   HFT Brokers have done the most TWPI, as well as violation
:::

```{r}
broker_10 <- rio_af |> 
  count(BuyerBrokerID) |>
  arrange(desc(n)) |>
  head(10) |>
  pull(BuyerBrokerID)

rio_af |> 
  filter(BuyerBrokerID %in% broker_10)|>
  filter(Marks != 1) |>
  group_by(Marks) |>
  count(BuyerBrokerID) |>
  arrange(desc(n)) |>
  ggplot(aes(x = reorder(BuyerBrokerID, -n), y = n, fill = Marks)) +
  geom_col(position = "dodge") +
  theme_bw() +
  labs(title = "Crossing Trades Reported as TWPI",
       x = "Broker ID",
       y = NULL) +
  theme(legend.position = "bottom")
```
:::
:::

## Result and Discussion - Modelling

```{r, include=FALSE}
# prepare and cleaning the data 

# data <- all_1pg_best |> 
#   left_join(twpi[, c("mykey", "code")], by = "mykey") |>
#   left_join(broker2[, c("No", "Description")], by = c("BuyerBrokerID" = "No")) |>
#   filter(RecordType %in% c("TRADE", "OFFTR")) |>
#   mutate(nxxt = ifelse(is.na(code), 0, 1),
#          violation = ifelse(code %in% c("at best", "outside"), 1, 0),
#          year = lubridate::year(RecordDate),
#          month = lubridate::month(RecordDate),
#          date = lubridate::day(RecordDate),
#          dayofweek = lubridate::wday(RecordDate), 
#          hour = lubridate::hour(hms)) |>
#   select(7:8, 10, 20, 27:35)
# 
# data_twpi <- data |>
#   filter(nxxt == 1,
#          !(month %in% c(1,2,3))) |> # month 1 to 3 only have 1 observation
#   mutate(across(6:10, as.factor),
#          across(12:13, as.factor)) |>
#   mutate_if(is.numeric, function(x) (x-mean(x))/sd(x)) |>
#   select(Price, Volume, DollarValue, Description, month, date, dayofweek, hour, violation) |>
#   na.omit()

data <- rio_af |> 
  filter(BuyerBrokerID %in% c(210,361,203,150,231),
         Hour != 16) |>
  # left_join(broker2[, c("No", "Description")], by = c("BuyerBrokerID" = "No")) |>
  mutate(Violation = as.factor(ifelse(Marks == 1, 0, 1))) |>
  select(2, 4, 6:13, 15) |>
  na.omit()

set.seed(1010)
df_split <- initial_split(data, 2/3, strata = Violation)
df_train <- training(df_split)
df_test <- testing(df_split)
```

```{r, eval=FALSE}
#logistic regression
log_mod <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification") |>
  translate()
log_fit <- log_mod |>
  fit(Violation ~ .,
      data = df_train[-1])

saveRDS(log_fit, file = "data/model/log_fit.rds")


tidy(log_fit) 
glance(log_fit)
```

```{r}
log_fit <- readRDS("data/model/log_fit.rds")
```

```{r,include=FALSE}
# measure accuracy logistic regression
treshold <- 0.05

df_tr_pred <- log_fit |> 
  augment(new_data = df_train) |>
  mutate(pred_viol = as.factor(ifelse(.pred_1 > treshold, 1, 0)))
df_ts_pred <- log_fit |> 
  augment(new_data = df_test) |>
  mutate(pred_viol = as.factor(ifelse(.pred_1 > treshold, 1, 0)))

matrix_tr <- df_tr_pred |>
  count(Violation, pred_viol) |>
  group_by(Violation) |>
  mutate(cl_acc = n[pred_viol == Violation]/sum(n)) |>
  pivot_wider(names_from = pred_viol, 
              values_from = n, values_fill=0)

matrix_ts <- df_ts_pred |>
  count(Violation, pred_viol) |>
  group_by(Violation) |>
  mutate(cl_acc = n[pred_viol == Violation]/sum(n)) |>
  pivot_wider(names_from = pred_viol, 
              values_from = n, values_fill=0)

accuracy(df_tr_pred, Violation, pred_viol)$.estimate
bal_accuracy(df_tr_pred, Violation, pred_viol)$.estimate

accuracy(df_ts_pred, Violation, pred_viol)$.estimate
bal_accuracy_logistic <- bal_accuracy(df_ts_pred, Violation, pred_viol)$.estimate
```

```{r, include = FALSE}
# # LDA
# lda_mod <- discrim_linear() |>
#   set_mode("classification") |>
#   set_engine("MASS", prior = c(0.5, 0.5))
# lda_fit <- lda_mod |> 
#   fit(Violation ~ .,
#       data = df_train[-1])
# 
# lda_fit$fit$scaling
```

```{r, include = FALSE}
# # measure accuracy LDA
# 
# df_tr_pred_lda <- df_train |>
#   mutate(pred_viol = predict(lda_fit$fit, df_train)$class)
# df_ts_pred_lda <- df_test |>
#   mutate(pred_viol = predict(lda_fit$fit, df_test)$class)
# 
# matrix_tr_lda <- df_tr_pred_lda |>
#   count(violation, pred_viol) |>
#   group_by(violation) |>
#   mutate(cl_acc = n[pred_viol == violation]/sum(n)) |>
#   pivot_wider(names_from = pred_viol, 
#               values_from = n, values_fill=0)
# 
# matrix_ts_lda <- df_ts_pred_lda |>
#   count(violation, pred_viol) |>
#   group_by(violation) |>
#   mutate(cl_acc = n[pred_viol == violation]/sum(n)) |>
#   pivot_wider(names_from = pred_viol, 
#               values_from = n, values_fill=0)
# 
# accuracy(df_tr_pred_lda, violation, pred_viol)$.estimate
# bal_accuracy(df_tr_pred_lda, violation, pred_viol)$.estimate
# 
# accuracy(df_ts_pred_lda, violation, pred_viol)$.estimate
# bal_accuracy_lda <- bal_accuracy(df_ts_pred_lda, violation, pred_viol)$.estimate
```

```{r, eval = FALSE}
# decision tree
tree <- decision_tree(
  cost_complexity = 1e-10,
  tree_depth = 10,
  min_n = 4) |>
  set_mode("classification") |>
  set_engine("rpart")

fit_tree <- tree |>
  fit(Violation ~ .,
      data = df_train[-1])

saveRDS(fit_tree, file = "data/model/fit_tree.rds")

# fit_tree |>
#   extract_fit_engine() |>
#   rpart.plot(type=3, extra=1)
```

```{r}
fit_tree <- readRDS("data/model/fit_tree.rds")
```

```{r, eval=FALSE}
tree_tune <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) |>
  set_mode("classification") |>
  set_engine("rpart")

# create all combination with 5 samples each variable 
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(),
                          levels = 5)

# create cross-validation folds from training set
d_folds <- vfold_cv(df_train)

# tuning the model with all the combination
tree_wf <- workflow() %>%
  add_model(tree_tune) %>%
  add_formula(Violation ~ . -Instrument)

tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = d_folds,
    grid = tree_grid
    )

# select best combination based on accuracy
best_tree <- tree_res |> select_best(metric = "roc_auc")
```

```{r, include=FALSE}
# measure accuracy decision tree
df_tr_pred_tree <- df_train |>
  mutate(pviol = predict(fit_tree$fit, 
                            df_train, 
                            type="class"))

df_ts_pred_tree <- df_test |>
  mutate(pviol = predict(fit_tree$fit, 
                            df_test, 
                            type="class"))

cf_tr <- df_tr_pred_tree |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n)


cf_ts <- df_ts_pred_tree |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n)

accuracy(df_tr_pred_tree, Violation, pviol)
bal_accuracy(df_tr_pred_tree, Violation, pviol)

accuracy(df_ts_pred_tree, Violation, pviol)
bal_accuracy(df_ts_pred_tree, Violation, pviol)
```

```{r, eval=FALSE}
# random forest
set.seed(1010)
rf <- rand_forest(mtry=2, trees=1000) |>
  set_mode("classification") |>
  set_engine("randomForest", importance = TRUE)

rf_workflow <- workflow() |>
  add_model(rf) |>
  add_formula(Violation ~ . -Instrument)

fit_rf <- rf_workflow |> 
  fit(data = df_train)

# fit_rf <- rf |>
#   fit(Violation ~ . -Price,
#       data = df_train)

df_ts_pred_rf <- df_test |>
  mutate(pviol = predict(fit_rf,
                         df_test)$.pred_class)

cf_rf_ts <- df_ts_pred_rf |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n, values_fill = 0)

accuracy(df_ts_pred_rf, Violation, pviol)
bal_accuracy_rf <- bal_accuracy(df_ts_pred_rf, Violation, pviol)$.estimate


rf_model <- pull_workflow_fit(fit_rf)$fit
importance(rf_model)
```

```{r, eval = FALSE}
data_rec <- recipe(Violation ~ ., data = df_train) |>
  step_downsample(Violation) |>
  step_rm(Instrument)

data_prep <- prep(data_rec)
data_juice <- juice(data_prep)

saveRDS(data_prep, file = "data/model/data_prep.rds")

```

```{r}
data_prep <- readRDS("data/model/data_prep.rds")
data_juice <- juice(data_prep)
```


```{r, eval = FALSE}
# tuning hyperparameters
rf_tune <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()) |>
  set_mode("classification") |>
  set_engine("randomForest")

tune_wf <- workflow() |>
  add_recipe(data_rec) |>
  add_model(rf_tune)
  
doParallel::registerDoParallel() # for paralle processing

set.seed(3344)
w_folds <- vfold_cv(data)

rf_grid <- grid_regular(mtry(range = c(2, 8)),
                        min_n(range = c(1, 4)),
                        levels = 3)

tune_res <- 
  tune_grid(tune_wf,
            resamples = w_folds,
            grid = rf_grid
            )

# select best combination based on accuracy
best_hp <- tune_res |> select_best(metric = "accuracy")
```

```{r, eval = FALSE}
# apply the model
set.seed(3344)
rf <- rand_forest(mtry = 5, 
                     min_n = 4) |>
  set_mode("classification") |>
  set_engine("randomForest", importance = TRUE)

rf_workflow2 <- workflow() |>
  add_model(rf) |>
  add_formula(Violation ~ .)

fit_rf2 <- rf_workflow2 |> 
  fit(data = data_juice)

saveRDS(fit_rf2, file = "data/model/fit_rf2.rds")
```

```{r}
fit_rf2 <- readRDS("data/model/fit_rf2.rds")
```

```{r, eval = FALSE}
df_ts_pred_rf2 <- bake(data_prep, df_test) |>
  mutate(pviol = predict(fit_rf2,
                         bake(data_prep, df_test))$.pred_class)

cf_rf_ts2 <- df_ts_pred_rf2 |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n, values_fill = 0)

save(df_ts_pred_rf2, file = "data/model/df_ts_pred_rf2.csv")
```

```{r}
load("data/model/df_ts_pred_rf2.csv")
```

```{r, include = FALSE}
accuracy(df_ts_pred_rf2, Violation, pviol)
bal_accuracy_rf2 <- bal_accuracy(df_ts_pred_rf2, Violation, pviol)$.estimate

rf_model2 <- pull_workflow_fit(fit_rf2)$fit
importance(rf_model2)
```

```{r, eval=FALSE}
# boosted tree
bt <- boost_tree() |>
  set_mode("classification") |>
  set_engine("xgboost")
fit_bt <- bt |>
  fit(Violation ~ .,
      data = df_train[-1])

df_ts_pred_rf <- df_test |>
  mutate(pviol = predict(fit_bt, 
                            df_test)$.pred_class)

cf_bt_ts <- df_ts_pred_rf |>
  count(Violation, pviol) |>
  group_by(Violation) |>
  mutate(Accuracy = n[Violation==pviol]/sum(n)) |>
  pivot_wider(names_from = "pviol", 
              values_from = n, values_fill = 0)

bt_acc_ts <- accuracy(df_ts_pred_rf, Violation, pviol)
bt_bacc_ts <- bal_accuracy(df_ts_pred_rf, Violation, pviol)
rbind(bt_acc_ts, bt_bacc_ts)
```

For machine learning modelling to understand violation pattern:

-   Logistic Regression
-   Decision Tree
-   Random Forest
-   Boosted Tree (xgboost)

Random Forest gives the best result for this classification problem. Highest balance accuracy score on the test set.

## Result and Discussion - Modelling

::: columns
::: {.column width="40%"}
```{r}
matrix_ts |>
  kbl() |>
  kable_styling(font_size = 18, full_width = F, position = "left") |>
  add_header_above(c("Logistic Regression" = ncol(matrix_ts)))
```

::: {style="font-size: 0.75em;"}
```{r, echo = TRUE}
bal_accuracy_logistic
```
:::

```{r}
cf_rf_ts2 |>
  kbl() |>
  kable_styling(font_size = 18, full_width = F, position = "left") |>
  add_header_above(c("Random Forest" = ncol(matrix_ts)))
```

::: {style="font-size: 0.75em;"}
```{r, echo = TRUE}
bal_accuracy_rf2
```
:::
:::

::: column
```{r}
# tidy(log_fit)[,1:2] |>
#   left_join(data.frame(lda_fit$fit$scaling) |> rownames_to_column(),
#             by = c("term" = "rowname")) |>
#   rename(logreg_est = estimate,
#          lda_coef = LD1) |>
#   kbl() |>
#   kable_styling(font_size = 15, full_width = F, position = "left")
```

```{r}
importance(rf_model2)
```

:::
:::

## Conclusion

- The purpose of TWPI has been met as the number of transactions at the same price as the market price drastically reduced. 
-   Violation of TWPI is mostly correlated with the depth of the liquidity  available just before the trades (Ask and Bid Volume available in the open market). Date, Volume, and the transaction values also play important role in the TWPI violation pattern.
-   There is no measure from ASX to reject TWPI from being reported if the condition is not met -> ASX need to address this issue

## Future Work

- Analyse pattern with other variables, such as stock's industry sector, market cap, lagged price values, bid-ask spread, etc.
- How much profit did ASX make due to the change in the TWPI rule?




